# Other ways to improve grading

## Allow reclaiming correction issues.

We need fair, unbiased and precise grading, but we now that AI (and humans) are far from perfect.
So, we propose offering a mechanism to appeal any unfair correction.
This will not only become a feedback loop to improve the AI
but could also help to identify ambiguous or misleading questions.

## Beta test new questions as part of the exam training process

Any important test end up having a market for training.
We can offer training tests for third party academies and universities with new questions to beta test the
automatic correction.
The trainers could manually review the automatic AI grading and provide feedback.

## Stop grading after a clear failure

Candidates need 80% to pass the test. What would happen if the grading could stop after identifying 20% of failures?
This can destroy value for candidates that typically want feedback and explanations about their mistakes.

This change would mean changing the requirements.
But architects can provide original solutions by challenging the initial assumptions.
Of course, the effect of this measure depends on the of pass vs. failed candidates metric that we do not have. 